from cmath import nan
import torch
import torch.nn as nn

# 输入随机向量作为种子?
class Gate(nn.Module):
    def __init__(self, input_dim):
        super(Gate, self).__init__()
        self.linear = nn.Linear(input_dim, 2)
        self.softmax = nn.Softmax(1)
    
    def forward(self, x):
        x = self.linear(x)
        x = self.softmax(x)
        return x

class Generator(nn.Module):
    """ Generator """
    def __init__(self, env):
        super(Generator, self).__init__()
        self.env = env
        #self.poi_dur_dist = env.poi_dur_dist
        #self.eid_poi = env.eid_poi
        self.all_time_attraction = env.attraction
        self.connectivity = env.connectivity
        self.random_size = env.gen_random_size
        self.hidden_dim = env.gen_hidden_dim
        self.node_embed = nn.Embedding(env.num_nodes, env.node_embedding_dim)
        self.lstm = nn.LSTM(env.node_embedding_dim + env.gen_random_size, 32, batch_first=True)
        self.FC1 = nn.Linear(32, 32)
        self.FC2 = nn.Linear(32, 64)
        self.FC3 = nn.Linear(64, env.num_nodes)
        self.log_softmax = nn.LogSoftmax(dim=1)
        self.softmax = nn.Softmax(dim=1)
        self.tanh = nn.Tanh()
        self.relu = nn.LeakyReLU(0.01)
        self.init_params()
        self.true_tensor = torch.ones(1, device=self.env.device).squeeze()
        self.false_tensor = torch.zeros(1, device=self.env.device).squeeze()
        self.idx_s_t_tensor = env.idx_s_t_tensor
        self.masked = True

    def forward(self, x):
        """
        Embeds input and applies LSTM on the input sequence.
        Inputs: x
            - x: (batch_size, seq_len), sequence of tokens generated by generator
        Outputs: out
            - out: (batch_size * seq_len, vocab_size), lstm output prediction
        """
        self.lstm.flatten_parameters()
        node_emb = self.node_embed(x) # batch_size * seq_len * node_emb_dim
        r = torch.rand(node_emb.size(0), node_emb.size(1), self.random_size, device=self.env.device)
        lstm_input = torch.cat((node_emb, r), dim=2) # batch_size * seq_len * (node_emb_dim + rand_dim)
        lstm_output, _ = self.lstm(lstm_input) # out: batch_size * seq_len * hidden_dim
        print(lstm_output.size())
        hidden = self.FC1(lstm_output) # batch_size * seq_len * num_node
        hidden = self.tanh(hidden)
        hidden = self.FC2(hidden)
        hidden = self.tanh(hidden)
        hidden = self.FC3(hidden)

        node_dist = self.softmax(hidden, dim=2)
        
        log_node_dist = torch.log(node_dist) # batch_size * seq_length * node_count
        return log_node_dist

    def step(self, x, h, c, r):
        """
        Embeds input and applies LSTM one token at a dur (seq_len = 1).
        
        Inputs: x, h, c, node_mask, dur_mask
            - x: (batch_size, 1), sequence of tokens generated by generator
            - h: (1, batch_size, hidden_dim), lstm hidden state
            - c: (1, batch_size, hidden_dim), lstm cell state

        Outputs: next_node, next_duration, log_node_dist, log_dur_dist, h, c
            - next_node: (batch_size, 1), the next node of each traj
            - log_node_dist: (batch_size, 1, num_node), logged next node probablity distribution 
            - h: (1, batch_size, hidden_dim), lstm hidden state
            - c: (1, batch_size, hidden_dim), lstm cell state 
        """
        self.lstm.flatten_parameters()
        if torch.isnan(torch.sum(self.node_embed.weight)):
            for i in range(2751):
                if torch.isnan(torch.sum(self.node_embed.weight[i])):
                    print(str(i) + " is nan")
        node_emb = self.node_embed(x) # batch_size * 1 * node_emb_dim
        lstm_input = torch.cat((node_emb, r.unsqueeze(1)), dim=2) # batch_size * 1 * (node_emb_dim + rand_dim)
        lstm_output, (h, c) = self.lstm(lstm_input, (h, c)) # out: batch_size * 1 * hidden_dim

        # todo: rewrite get node mask
        hidden = self.FC1(lstm_output.squeeze()) # batch_size * num_node
        hidden = self.tanh(hidden)
        hidden = self.FC2(hidden)
        hidden = self.tanh(hidden)
        hidden = self.FC3(hidden)

        if self.masked:
            node_mask = self.get_node_mask(x[:, 0]) 
            hidden = hidden.masked_fill_((1 - node_mask).bool(), -10)

        node_dist = self.softmax(hidden) + 1e-6 # batch * num_node
        next_node = torch.multinomial(node_dist, 1)
        
        log_node_dist = None
        if self.env.training_gen:
            log_node_dist = torch.log(node_dist).unsqueeze(1) # batch_size * node_count
        
        return next_node, log_node_dist, h, c
        
    def init_hidden(self, batch_size):
        h = torch.zeros(1, batch_size, self.hidden_dim, device=self.env.device)
        c = torch.zeros(1, batch_size, self.hidden_dim, device=self.env.device)
        return h, c
    
    def init_params(self):
        for param in self.parameters():
            param.data.uniform_(-0.05, 0.05)

    def sample(self, seq_len, trajs, rand_var, start_time, start_node_dist):
        tmp = self.parameters()
        for para in tmp:
            para
        """
        Samples the network and returns a batch of samples of length seq_len.\n
        
        算法说明：
        每个batch的起始点服从同一个概率分布，1个epoch
        需要统计入场时间和轨迹长度的联合分布（M * N矩阵，M为时间，N为轨迹长度），每个batch选取一个时间，即每个batch轨迹起始时间相同，随机采样长度

        Inputs:\n
            - seq_len: sequence length\n
            - trajs: sequence prefix (batch, length), node time tuple\n
            - rand_var: (batch_size, random_size), random variable as gan inputs
            - start_time: (batch_size, ), start time (minute idx)
            - start_node_dist: (batc_size, num_nodes), start node distribution

        Outputs:\n
            - samples: (batch, seq_len)\n
            - log_node_prob: (batch * seq_len, node_count)\n
        """
        self.current_time = start_time.clone() * 5
        samples = []
        log_node_probs = []

        size = trajs.size()
        batch_size = size[0]
        if self.env.training_gen:
            start_node_dist_ = torch.zeros(batch_size, 1, self.env.num_nodes, device=self.env.device)
            start_node_dist_[:, 0, :] = start_node_dist
            log_node_probs.append(torch.log(start_node_dist_))
        h, c = self.init_hidden(batch_size)
        given_len = size[1]
        stays = trajs.chunk(given_len, dim=1)
        
        for i in range(given_len):
            if i == 0:
                samples = stays[i]
            else:
                samples = torch.cat((samples, stays[i]), dim=1)
            next_node, log_node_dist, h, c = self.step(stays[i], h, c, rand_var)

            if self.env.training_gen:
                log_node_probs.append(log_node_dist)

        for i in range(given_len, seq_len - 1):
            samples = torch.cat((samples, next_node), dim=1)
            next_node, log_node_dist, h, c = self.step(next_node, h, c, rand_var)

            if self.env.training_gen:
                log_node_probs.append(log_node_dist)
            if i == seq_len - 2:
                samples = torch.cat((samples, next_node), dim=1)
        if self.env.training_gen:
            #log_node_probs.pop()
            log_node_probs = torch.cat(log_node_probs, dim=1).contiguous().view(-1, self.env.num_nodes)

        return samples, log_node_probs

    def get_node_mask(self, cur_step):
        """
        Filter out impossible node options (topologically impossible):
        
        Inputs:
            - cur_step: (batch_size, 1), current step
        
        Output:
            - node_mask: (batch_size, num_nodes)
        """
        accessible_place = self.connectivity[cur_step.squeeze(), :] # batch_size * num_nodes
        return accessible_place