import torch
import torch.nn as nn

class Generator(nn.Module):
    """ Generator """

    def __init__(self, vocab_size, embedding_dim, hidden_dim, use_cuda, acc_mat, flag_embd_dim, start_flag):
        super(Generator, self).__init__()
        self.hidden_dim = hidden_dim
        self.use_cuda = use_cuda
        self.node_embed = nn.Embedding(vocab_size, embedding_dim)
        self.state_embed = nn.Embedding(5, flag_embd_dim)
        self.lstm = nn.LSTM(embedding_dim + flag_embd_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, vocab_size)
        self.log_softmax = nn.LogSoftmax(dim=1)
        self.softmax = nn.Softmax(dim=1)

        self.start_flag = start_flag
        self.acc_mat = acc_mat
        self.rollout = False
        self.masked = False
        self.init_params()

    def forward(self, x):
        """
        Embeds input and applies LSTM on the input sequence.
        Inputs: x
            - x: (batch_size, seq_len), sequence of tokens generated by generator
        Outputs: out
            - out: (batch_size * seq_len, vocab_size), lstm output prediction
        """
        state_idx = self.init_flag(x.size())        
        for i in range(1, x.size(1)):
            floor1_to_floor2 = torch.where(x[:, i] == 6)[0].squeeze()
            floor2_to_floor1 = torch.where(x[:, i] == 7)[0].squeeze()
            state_idx[floor1_to_floor2, i:-2] = 2
            state_idx[floor2_to_floor1, i:-2] = 1

        self.lstm.flatten_parameters()
        h0, c0 = self.init_hidden(x.size(0))
        node_emb = self.node_embed(x) # batch_size * seq_len * emb_dim 
        state_emb = self.state_embed(state_idx)
        out, _ = self.lstm(torch.cat((node_emb, state_emb), dim=2), (h0, c0)) # out: batch_size * seq_len * hidden_dim
        out = self.log_softmax(self.fc(out.contiguous().view(-1, self.hidden_dim))) # (batch_size*seq_len) * vocab_size
        return out

    def step(self, x, h, c, seq_len, idx):
        """
        Embeds input and applies LSTM one token at a time (seq_len = 1).
        Inputs: x, h, c
            - x: (batch_size, 1), sequence of tokens generated by generator
            - h: (1, batch_size, hidden_dim), lstm hidden state
            - c: (1, batch_size, hidden_dim), lstm cell state
        Outputs: out, h, c
            - out: (batch_size, vocab_size), lstm output prediction
            - h: (1, batch_size, hidden_dim), lstm hidden state
            - c: (1, batch_size, hidden_dim), lstm cell state 
        """
        state_idx = torch.ones(x.size(0), dtype=torch.long, device=torch.device('cuda:0')).detach()
        if idx == 1:
            self.previous_state = torch.ones(x.size(0), dtype=torch.long, device=torch.device('cuda:0'))
        if idx == 0:
            state_idx[:] = 0
        elif idx < seq_len - 2:
            state_idx = torch.clone(self.previous_state)
            self.update_state(state_idx, x)
        elif idx == seq_len - 1:
            state_idx[:] = 3
        self.previous_state = torch.clone(state_idx)
        if len(state_idx.size()) == 1:
            state_idx = state_idx.unsqueeze(1)

        self.lstm.flatten_parameters()
        node_emb = self.node_embed(x) # batch_size * 1 * emb_dim
        state_emb = self.state_embed(state_idx)
        
        out, (h, c) = self.lstm(torch.cat((node_emb, state_emb), dim=2), (h, c)) # out: batch_size * 1 * hidden_dim
        out = self.fc(out.contiguous().view(-1, self.hidden_dim)) # batch_size * node_size
        #out[:, self.vocab_size] = -10
        topo_mask = self.acc_mat[x.view(-1)] # batch_size * node_size
        if self.masked:
            out *= topo_mask
            out += -1e3 * (1 - topo_mask)
        prob = self.softmax(out) + 1e-6
        topo_err = prob * (1- topo_mask) # batch_size * node_size
        dist = prob
        log_prob = torch.log(prob) # batch_size * node_size
        return log_prob, h, c, dist, topo_err

    def init_hidden(self, batch_size):
        h = torch.zeros(1, batch_size, self.hidden_dim)
        c = torch.zeros(1, batch_size, self.hidden_dim)
        if self.use_cuda:
            h, c = h.cuda(), c.cuda()
        return h, c
    
    def init_params(self):
        for param in self.parameters():
            param.data.uniform_(-0.05, 0.05)

    def sample(self, batch_size, seq_len, x=None):
        """
        Samples the network and returns a batch of samples of length seq_len.
        Outputs: out
            - out: (batch_size * seq_len)
        """
        samples = []
        topo_errs = []
        node_dist = []
        if x is None:
            self.previous_state = torch.zeros(batch_size, dtype=torch.long, device=torch.device('cuda:0'))
            h, c = self.init_hidden(batch_size)
            x = torch.ones(batch_size, 1, dtype=torch.int64) * self.start_flag
            if self.use_cuda:
                x = x.cuda()
            for i in range(seq_len):
                log_prob, h, c, dist, topo_err = self.step(x, h, c, seq_len, i)
                prob = torch.exp(log_prob)
                x = torch.multinomial(prob, 1)
                samples.append(x)
                node_dist.append(dist)
                topo_errs.append(topo_err)
        else:
            self.previous_state = torch.ones(batch_size, dtype=torch.long, device=torch.device('cuda:0'))
            h, c = self.init_hidden(x.size(0))
            given_len = x.size(1)
            lis = x.chunk(x.size(1), dim=1)
            for i in range(0, given_len):
                out, h, c, _, _ = self.step(lis[i], h, c, seq_len, i + 1)
                samples.append(lis[i])
            prob = torch.exp(out)
            x = torch.multinomial(prob, 1)
            samples.append(x)
            for i in range(given_len + 1, seq_len):
                out, h, c, _, _ = self.step(x, h, c, seq_len, i)
                prob = torch.exp(out)
                x = torch.multinomial(prob, 1)
                samples.append(x)
        out = torch.cat(samples, dim=1) # along the batch_size dimension
        node_dist = torch.stack(node_dist, dim=1)
        topo_errs = torch.stack(topo_errs, dim=1)
        return out, node_dist, topo_errs

    def init_flag(self, size):
        flag_idx = torch.ones(size, dtype=torch.long, device=torch.device('cuda:0'))
        flag_idx[:, -1] = 4
        flag_idx[:, -2] = 3
        flag_idx[:, 0] = 0
        return flag_idx
    
    def update_state(self, state_idx, x):
        x = x.view(-1)
        floor1_to_floor2 = torch.where(x == 6)[0].squeeze() # 0: change floor
        floor2_to_floor1 = torch.where(x == 7)[0].squeeze()
        state_idx[floor1_to_floor2] = 2
        state_idx[floor2_to_floor1] = 1